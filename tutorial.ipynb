{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2f51c42",
   "metadata": {},
   "source": [
    "# Tutorial for auto differentiation package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a133b62",
   "metadata": {},
   "source": [
    "This package created by @Faris-ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8677c0",
   "metadata": {},
   "source": [
    "### How to differentiate automaticlly?\n",
    "\n",
    "The derivative is taken by going throuth the following steps:\n",
    "\n",
    "1- Define the equation by the package operator\n",
    "\n",
    "2- Create a graph\n",
    "\n",
    "3- compute the forward pass and backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ffd456",
   "metadata": {},
   "source": [
    "### Methodology:\n",
    "\n",
    "The way to be able to take derivative is to convert the equation to graph.\n",
    "\n",
    "example:\n",
    "\n",
    "let say that we want to evaluate this equation\n",
    "\n",
    "y = (x^2)+6\n",
    "\n",
    "after converting the equation to graph will be like this:\n",
    "\n",
    "![image](images/graph.png)\n",
    "\n",
    "and by the chain rule we compute the gradients by taking the backward pass like the graph below :\n",
    "\n",
    "![image](images/d_graph.png)\n",
    "\n",
    "## lets get started on implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4791c13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ElhamMath import Tensor, mul, Constant, Variable, divide, exp, add, Graph\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e5e23a",
   "metadata": {},
   "source": [
    "Define variables and constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc29d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: Union[Variable, Constant]):\n",
    "    # Ïƒ(x) = 1 / (1 + exp(-x))\n",
    "    one = Constant(Tensor(1.0), \"ones\")\n",
    "    negx = mul(Constant(Tensor(-1.0), \"neg_ones\"), x, \"neg_mul\")\n",
    "    return divide(one, add(one, exp(negx), \"add\"), \"devide\")\n",
    "\n",
    "\n",
    "B, C, H, W = 2, 3, 4, 5\n",
    "data = [\n",
    "    [\n",
    "        [[(i - 0.5 * j + 0.1 * k - 0.2 * l) for l in range(W)] for k in range(H)]\n",
    "        for j in range(C)\n",
    "    ]\n",
    "    for i in range(B)\n",
    "]\n",
    "\n",
    "x = Variable(Tensor(data), \"x\")\n",
    "y = sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630ba58f",
   "metadata": {},
   "source": [
    "Create a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d3c2258",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Graph(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5a618e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39d10b5",
   "metadata": {},
   "source": [
    "Compute forward pass and backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "807fc398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the forward pass :  [0.5, 0.45016600268752216, 0.401312339887548, 0.35434369377420455, 0.31002551887238755, 0.52497918747894, 0.47502081252106, 0.425557483188341, 0.3775406687981454, 0.3318122278318339, 0.549833997312478, 0.5, 0.45016600268752216, 0.401312339887548, 0.35434369377420455, 0.574442516811659, 0.52497918747894, 0.47502081252106, 0.425557483188341, 0.3775406687981454, 0.3775406687981454, 0.3318122278318339, 0.289050497374996, 0.24973989440488234, 0.2141650169574414, 0.401312339887548, 0.35434369377420455, 0.31002551887238755, 0.2689414213699951, 0.23147521650098232, 0.425557483188341, 0.3775406687981454, 0.3318122278318339, 0.289050497374996, 0.24973989440488234, 0.45016600268752216, 0.401312339887548, 0.35434369377420455, 0.31002551887238755, 0.2689414213699951, 0.2689414213699951, 0.23147521650098238, 0.19781611144141825, 0.16798161486607552, 0.14185106490048777, 0.289050497374996, 0.24973989440488234, 0.2141650169574414, 0.18242552380635635, 0.15446526508353467, 0.31002551887238755, 0.2689414213699951, 0.23147521650098232, 0.19781611144141822, 0.16798161486607552, 0.3318122278318339, 0.28905049737499605, 0.24973989440488234, 0.2141650169574414, 0.18242552380635635, 0.7310585786300049, 0.6899744811276125, 0.6456563062257954, 0.598687660112452, 0.5498339973124778, 0.7502601055951177, 0.710949502625004, 0.6681877721681662, 0.6224593312018546, 0.574442516811659, 0.7685247834990175, 0.7310585786300049, 0.6899744811276125, 0.6456563062257954, 0.598687660112452, 0.7858349830425586, 0.7502601055951177, 0.7109495026250039, 0.6681877721681662, 0.6224593312018546, 0.6224593312018546, 0.574442516811659, 0.5249791874789399, 0.47502081252106, 0.425557483188341, 0.6456563062257954, 0.598687660112452, 0.5498339973124778, 0.5, 0.45016600268752216, 0.6681877721681662, 0.6224593312018546, 0.574442516811659, 0.5249791874789399, 0.47502081252106, 0.6899744811276125, 0.6456563062257954, 0.598687660112452, 0.5498339973124778, 0.5, 0.5, 0.45016600268752216, 0.401312339887548, 0.35434369377420455, 0.31002551887238755, 0.52497918747894, 0.47502081252106, 0.425557483188341, 0.3775406687981454, 0.3318122278318339, 0.549833997312478, 0.5, 0.45016600268752216, 0.401312339887548, 0.35434369377420455, 0.574442516811659, 0.52497918747894, 0.47502081252106, 0.425557483188341, 0.3775406687981454]\n",
      "the derivative with respect to x is :  [0.25, 0.24751657271186003, 0.24026074574152917, 0.22878424045665727, 0.21390969652029443, 0.24937604019289195, 0.24937604019289197, 0.24445831169074586, 0.23500371220159452, 0.22171287329310904, 0.24751657271185995, 0.25, 0.24751657271186003, 0.24026074574152917, 0.22878424045665727, 0.2444583116907459, 0.24937604019289195, 0.24937604019289195, 0.24445831169074586, 0.2350037122015945, 0.2350037122015945, 0.22171287329310904, 0.20550030734226343, 0.18736987954752055, 0.16829836246906021, 0.24026074574152917, 0.22878424045665727, 0.21390969652029443, 0.19661193324148185, 0.17789444064680568, 0.24445831169074586, 0.2350037122015945, 0.22171287329310904, 0.20550030734226343, 0.18736987954752055, 0.24751657271186003, 0.24026074574152917, 0.22878424045665724, 0.21390969652029443, 0.19661193324148185, 0.19661193324148185, 0.1778944406468057, 0.15868489749561465, 0.139763791933061, 0.12172934028708537, 0.20550030734226343, 0.18736987954752055, 0.16829836246906021, 0.14914645207033286, 0.13060574696620805, 0.21390969652029443, 0.19661193324148185, 0.17789444064680568, 0.15868489749561462, 0.139763791933061, 0.22171287329310904, 0.20550030734226346, 0.18736987954752055, 0.16829836246906021, 0.14914645207033286, 0.19661193324148188, 0.21390969652029443, 0.2287842404566573, 0.24026074574152914, 0.24751657271185995, 0.1873698795475206, 0.20550030734226343, 0.22171287329310907, 0.2350037122015945, 0.2444583116907459, 0.17789444064680573, 0.19661193324148188, 0.21390969652029446, 0.2287842404566573, 0.24026074574152914, 0.16829836246906024, 0.1873698795475206, 0.20550030734226343, 0.2217128732931091, 0.2350037122015945, 0.2350037122015945, 0.2444583116907459, 0.24937604019289192, 0.24937604019289197, 0.24445831169074586, 0.2287842404566573, 0.24026074574152914, 0.24751657271185995, 0.25, 0.24751657271186003, 0.2217128732931091, 0.2350037122015945, 0.2444583116907459, 0.24937604019289195, 0.24937604019289197, 0.21390969652029443, 0.22878424045665727, 0.24026074574152914, 0.24751657271185995, 0.25, 0.25, 0.24751657271186003, 0.24026074574152917, 0.22878424045665727, 0.21390969652029443, 0.24937604019289195, 0.24937604019289197, 0.24445831169074586, 0.23500371220159452, 0.22171287329310904, 0.24751657271185995, 0.25, 0.24751657271186003, 0.24026074574152917, 0.22878424045665727, 0.2444583116907459, 0.24937604019289195, 0.24937604019289195, 0.24445831169074586, 0.2350037122015945]\n",
      "0.9984970092773438\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "forward = graph.forward()\n",
    "print(\"the forward pass : \", forward.data)\n",
    "graph.backward()\n",
    "print(\"the derivative with respect to x is : \", x.grad.data)\n",
    "e = time.time()\n",
    "print((e - s) * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "217cda6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, List, Union\n",
    "from ElhamMath import Tensor, Variable, Constant, exp, mul, divide, sub, sqrt, matmul, Node, power, Graph, add\n",
    "\n",
    "\n",
    "# ----- activations -----\n",
    "def sigmoid(x: Node):\n",
    "    # Ïƒ(x) = 1 / (1 + exp(-x))\n",
    "    one = Constant(Tensor(1.0), \"ones\")\n",
    "    negx = mul(Constant(Tensor(-1.0), \"neg_ones\"), x, \"neg_mul\")\n",
    "    return divide(one, add(one, exp(negx), \"add\"), \"devide\")\n",
    "\n",
    "\n",
    "def tanh(x: Node):\n",
    "    # if you have a native tanh(), use it; otherwise use exp:\n",
    "    epos = exp(x)\n",
    "    eneg = exp(mul(Constant(Tensor(-1),\"tanh\"),x))\n",
    "    return divide(sub(epos,eneg),add(epos,eneg))\n",
    "\n",
    "\n",
    "def relu(x: Node):\n",
    "    half = Constant(Tensor(0.5),\"relu1\")\n",
    "    eps = Constant(Tensor(1e-12),\"relu2\")\n",
    "    return mul(half,add(x,sqrt(add(mul(x,x),eps))))  # approx |x| -> (x+|x|)/2\n",
    "\n",
    "\n",
    "# ----- layers -----\n",
    "class Linear:\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        std: float = 0.01,\n",
    "    ):\n",
    "        import random\n",
    "\n",
    "        W_py = [\n",
    "            [random.gauss(0.0, std) for _ in range(out_features)]\n",
    "            for _ in range(in_features)\n",
    "        ]\n",
    "        b_py = [0.0 for _ in range(out_features)]\n",
    "        self.W = Variable(Tensor(W_py),\"W\")\n",
    "        self.b = Variable(Tensor(b_py),\"b\")\n",
    "\n",
    "    def __call__(self, x: Node):\n",
    "        y = add(matmul(x, self.W),self.b)  # broadcasting adds bias\n",
    "        return y\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return [self.W, self.b]\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        hidden_features: int,\n",
    "        out_features: int,\n",
    "    ):\n",
    "        self.l1 = Linear(in_features, hidden_features)\n",
    "        self.l2 = Linear(hidden_features, out_features)\n",
    "\n",
    "    def __call__(self, x: Node):\n",
    "        h = sigmoid(self.l1(x))  # swap for relu/tanh if you prefer\n",
    "        y = sigmoid(self.l2(h))  # for binary outputs\n",
    "        return y\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return self.l1.params + self.l2.params\n",
    "\n",
    "\n",
    "# ----- losses -----\n",
    "def mse_loss(y_pred: Node, y_true: Node):\n",
    "    err = power(sub(y_pred,y_true),Constant(Tensor(2),\"loss_power\"))\n",
    "    return err\n",
    "\n",
    "\n",
    "# def bce_loss(y_pred: Variable, y_true: Constant, eps: float = 1e-7):\n",
    "#     # Binary cross-entropy: -[y*log(p) + (1-y)*log(1-p)]\n",
    "#     p = y_pred\n",
    "#     one = 1.0\n",
    "#     loss_tensor = -(y_true * (p + eps).ln() + (one - y_true) * (one - p + eps).ln())\n",
    "#     return reduce_to_shape(loss_tensor, []) if reduce_to_shape else loss_tensor.sum()\n",
    "\n",
    "\n",
    "# ===== Example wiring =====\n",
    "# Toy data (N=4, D=2 -> binary label)\n",
    "X_py = [\n",
    "    [0.0, 0.0],\n",
    "    [0.0, 1.0],\n",
    "    [1.0, 0.0],\n",
    "    [1.0, 1.0],\n",
    "]\n",
    "Y_py = [\n",
    "    [0.0],\n",
    "    [1.0],\n",
    "    [1.0],\n",
    "    [0.0],\n",
    "]\n",
    "\n",
    "X = Constant(Tensor(X_py),\"x\")\n",
    "Y = Constant(Tensor(Y_py),\"y\")\n",
    "\n",
    "model = MLP(in_features=2, hidden_features=4, out_features=1)\n",
    "\n",
    "# Build the graph\n",
    "pred = model(X)  # activation functions + weights as Variables, X as Constant\n",
    "loss = mse_loss(pred, Y)  # or bce_loss(pred, Y)\n",
    "g = Graph(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16cdacbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[4, 1], device=Device.CPU, data=[[0.24487506552908433], [0.2551196574284689], [0.25516366884579317], [0.2449462829870329]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "080227be",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "449fc513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[2, 4], device=Device.CPU, data=[[3.9494740950173936e-05, 6.455336752157997e-06, 0.0001378081227428724, 8.167849802356636e-05], [3.939078921131868e-05, 6.438598401613453e-06, 0.00013641906167491087, 8.051873598420459e-05]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.l1.W.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
